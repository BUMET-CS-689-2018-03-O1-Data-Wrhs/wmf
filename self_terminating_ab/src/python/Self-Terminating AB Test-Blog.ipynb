{
 "metadata": {
  "name": "",
  "signature": "sha256:07a8bf223051afff7d0acf8a2373d53c23a78e2365b0109fc651bf973b368692"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At the Wikimedia Foundation we use A/B testing to optimize the number of donations per impressions from our fundraising banners.\n",
      "\n",
      "Although classical Null Hypthothesis tests are probably still the most widespread technique for online A/B tests, I would argue that they are not the most appropriate for the web optimization use case. Null Hypthothesis testing was designed for situations in which:\n",
      "\n",
      "1. the goal is to control the probability of declaring a difference in populations means when there is none\n",
      "\n",
      "Most academic scientists care about this because it corresponds to false discoveries, which are embarassing at best. But in the banner optimization setting, we actually don\u2019t care about declaring the banners different when they are not. When the banners are equivalent in terms of click through rates (CTR) there is no bad decision. It may set the designers down the wrong path, because they think that a change they made to the control banner is a big win. But the choice has no impact on the expected revenue. What we really want, is to control the probability of declaring the worse banner the winner when they are in fact different. \n",
      "\n",
      "\n",
      "2. the sample size is predetermined or fixed in advance\n",
      "\n",
      "When running web experiments, we can often choose how long to run the experiment. Null Hypothesis testing does not take advantage of this fact. Instead, the best practice is to calculate the sample size in advance of running the test. To determine the sample size, you need to have an estimate of the CTR of your control banner and decide on the minumum effect size you want to detect (MDE). You also need to choose the desired probability of declaring a difference in CTRs when there is none (significance or alpha) and the probability of not declaring a difference when there is one (power or beta). For more information, check out the Wikipedia [page](http://en.wikipedia.org/wiki/Sample_size_determination#Required_sample_sizes_for_hypothesis_tests) on smaple size determination. For an [online calculator](http://www.evanmiller.org/ab-testing/sample-size.html), see Evan Miller's blog.\n",
      "\n",
      "Ceteris paribus, the sample size increases as you decrease the MDE. Now imagine a scenario in which you decided you want to be able to detect a 5% difference in click through rates. If the new banner is actually 50% better than the control, you have set yourself up for losing a lot of revenue on the portion of traffic exposed to the the control. Instead of commiting to a sample size in advance, we would really like our testing procedure to have an adaptive sample size. The larger the true difference in CTRs the shorter it should run. \n",
      "\n",
      "It may be tempting (and many people have been tempted) to just do run multiple null hypothesis tests as the data are coming in and decide to stop the test early or let it keep running. Some analysts even look at the p-value as a function of the sample size and stop the test if they feel like the p-vlaue has converged. There can be many valid reason's for peeking at test results that trump maintaining staitical soundness. One exmaple is to check for glitches in the testing framework,but problem with peeeking is that it corrupts the test. The more often you peek the higher the effective probability of declaring a difference in CTRs when there is none.\n",
      "\n",
      "To summarize, we need a testing procedure that:\n",
      "\n",
      "1. takes advanatage of the fact that the amount of data we need to determine if the new banner is better than the control decreases as the difference increases, while\n",
      "\n",
      "2. controling the probability of declaring the worse banner to be the winner.\n",
      "\n",
      "What follows is a proposed method to meet these ends. It relies heavily on simulation and numeric bayesian statistics.\n",
      "\n",
      "\n",
      "\n",
      "As a warm up, consider the following algorithm (ALG_1):\n",
      "\n",
      "Evaluate a Null Hypothesis Test on your samples every n records until you reach significance or hit the predetermined sample size. If you stopped because you hit significance, declare the banner with the higher CTR at termination to be the winner. If you stopped because you hit the max sample size, declare the winner to be unknown.\n",
      "\n",
      "I stated above that this kind of peeking corrupts the classic null hypothesis test. When the peeking is ad hoc its very hard to measure its effect, but here the peeking is is done in a very controlled way and we can use simulation to answer questions like:\n",
      "\n",
      "(*) How does the probability of picking banner A change as a function of the percent lift in CTR that A gives over B.\n",
      "\n",
      "\n",
      "We can simulate impressions of banner B by sampling from a bernoulli distribution $Bernoulli(CTR_B)$. \n",
      "\n",
      "#In practice, we would not know CTR_B, but we can gather some samples and get an estimate CTR_BHAT and then sample from #Bernoulli(CTR_BHAT). Even better would be to form a confidence interval and use the lower bound, which I will explain later.\n",
      "\n",
      "We can emulate impressions from synthetic banner A with y percent lift over banner B, by sampling from $Bernoulli(CTR_B * (100+y)/100)$. For a given percent lift y, we can run the procedure described above k times to get an estimate for (*)\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "max_sample_size = compute_sample_size(control_CTR, mde, alpha, power) \n",
      "samples = []\n",
      "while p_value(samples) < alpha:\n",
      "    samples.append(get_samples(n))\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "To motivate the design choices made, lets Consider the following algorithm:\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "1. Run the Control Banner for a while and form a 1-alpha confidence interval on its CTR\n",
      "\n",
      "Most of the time the control is the called the control because you have run it before and you have historical data on how it performed. In practice, the statistics underlying most behaviour on the web are not stationary, so beware that the data used for your estimate is reflective of the conditions under which you will actually test the banners.\n",
      "\n",
      "\n",
      "2. Create synthetic treatment banners (data drawn from \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "import pandas as pd\n",
      "pd.options.display.mpl_style = 'default'\n",
      "import numpy as np\n",
      "from abtest_util import SimStream, DonationProb, EmpiricalDonationProb\n",
      "from bayesian_abtest import CredibilityABTest, CostABTest\n",
      "from nh_abtest import NHABTest, samples_per_branch_calculator\n",
      "from abstract_abtest import expected_results, expected_results_by_lift"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Ground Truth Click Through Rates\n",
      "p_B = DonationProb(0.20, [1,], [1.0,])\n",
      "p_A = DonationProb(0.23, [1,], [1.0,])\n",
      "\n",
      "# Estimate Control CTR From Historical Data\n",
      "n = 1000\n",
      "hist_data_B = SimStream(p_B).get_next_records(n)\n",
      "p_hat = EmpiricalDonationProb(hist_data_B, p_B.values)\n",
      "ci =  p_hat.p_donate_ci()\n",
      "print \"CI over control:\",  ci\n",
      "interval = round(1.0/ci[0])*50\n",
      "print \"Evalaution Interval\", interval\n",
      "\n",
      "#interest in lift vlaues:\n",
      "lifts = [-0.20, -0.10, -0.05, -0.025, -0.015,0, 0.015, 0.025, 0.05, 0.10, 0.20]\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set Up NH  AB Test\n",
      "mde = 0.05\n",
      "alpha = 0.05\n",
      "power = 0.95\n",
      "max_run = samples_per_branch_calculator(p_hat, mde, alpha, power)\n",
      "print max_run\n",
      "\n",
      "iters = 1000\n",
      "expected_results_by_lift(NHABTest,[SimStream(p_A), SimStream(p_B), 250, max_run, alpha], iters, p_hat, lifts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As an example, lets pick MDE=0.05, beta = 0.95, alpha = 0.5. Lets do a Null hypothesis every n = 250 records and repeat the simulation k = 1000 times. Here are the results: \n",
      "\n",
      "\n",
      "The first thing to notice is that the probability of declaring A to be the better than B when the lift is 0 is 0.18. This means that the effective significance level of alg1 is only 0.36, although the single null hypothesis tests where testing at significance 0.05. Here you have the corrupting power of peeking at p_vlaues in action! But this algorithm has its redeeming qualities. Look at the expected number of samples needed as a function of lift. We can detect a 10% lift with probability 0.988 with only an average of 3500 records, one tenth of the classicly determined sample size! \n",
      "\n",
      "To get a deeper understanding of what the effect of peeking is, lets try the extremes: testing at every new pair of records and testing only once:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set Up NH  AB Test\n",
      "mde = 0.05\n",
      "alpha = 0.05\n",
      "power = 0.95\n",
      "max_run = samples_per_branch_calculator(p_hat, mde, alpha, power)\n",
      "print max_run\n",
      "\n",
      "iters = 10000\n",
      "expected_results_by_lift(NHABTest,[SimStream(p_A), SimStream(p_B), 41070, max_run, alpha], iters, p_hat, lifts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we only tested once and after the classically computed number of records. We see that when the lift is 0, there is roughly a 5% chance of declaring a winner. So, "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set Up NH  AB Test\n",
      "mde = 0.05\n",
      "alpha = 0.05\n",
      "power = 0.95\n",
      "max_run = samples_per_branch_calculator(p_hat, mde, alpha, power)\n",
      "print max_run\n",
      "\n",
      "iters = 1000\n",
      "expected_results_by_lift(NHABTest,[SimStream(p_A), SimStream(p_B), 1, max_run, alpha], iters, p_hat, lifts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The algorithm I will describe next has the same form but we will swap out the Null Hypthesis test withh a different stopping criterium. Instead of stopping when the probability of the observed difference in means under the assumption that the banners have the same CTR is below our significance level alpha,  (wow, so convoluted) lets stop when the expected cost associated with our decision is low. This idea comes from Chris Stuchio's [recent post](http://www.bayesianwitch.com/blog/2014/bayesian_ab_test.html)\n",
      "\n",
      "\n",
      "In particular, there is something strange going on when we run the simulation at 0 lift. We are waiting around for the null hyopothesis to falsely declare significance. Wouldn't it be better to terminate the experiment when we are reasonably sure that the the banners where the same and that the expected cost of choosing one over the other was small?\n",
      "\n",
      "\n",
      "To achieve this objective, we will need to switch to the bayesian formalism, in which we treat our click through rates not as unknown fixed parmeters, but as random variables. For a primer on bayesian AB testing, I recommend Sergey Feldman's [blog post](http://engineering.richrelevance.com/bayesian-ab-tests/).\n",
      "\n",
      "Say that after collecting N samples, you find banner A has a higher empirical click through rate than banner B. \n",
      "\n",
      "\n",
      "Using the techiques described in Feldman's blog, we can form a numeric representation of the joint posterior distribution over the click through rates CTR_A and CTR_B. \n",
      "\n",
      "To decide whether to stop the test, we could see how much of a chance remains that B is better that A and stop if P(CTR_B < CTR_A) is below a threshold. The rub is that if the banners really where the same, then P(CTR_B < CTR_A) should be 0.5. We are in the same situation as before of waiting for a false result to terminate our test. To avoid this we can compute the expected difference in click through rates \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Better, yet, lets look at the expected \n",
      "\n",
      "To measure the cost of the decision \n",
      "\n",
      "To decide if we should stop the test and go with A, we can measure how much probability mass remains for the secenario \n",
      "\n",
      "calculate the expected absolute cost relative to choosing B. \n",
      "\n",
      "$E[max((CTR_B - CTR_A), 0)]$\n",
      "\n",
      "Chris Stucchio recently derived a [closed form solution](https://www.chrisstucchio.com/blog/2014/bayesian_ab_decision_rule.html) for the absolute expectd cost. Absolute cost in terms of expected clicks lost per impression can be hard to interpret. I find a statement like: \"the data suggests that the expected  \n",
      "\n",
      "$E[max((CTR_B - CTR_A)/ CTR_A, 0)]$\n",
      "\n",
      "There are closed form solutions for  There are closed form solutions for \n",
      "\n",
      "IF the cost is below our threshold we stop the test\n",
      "\n",
      "Since we are modeling a banner impression event as a bernoulli random variable, the number of donations in N impressions has a binomial distribution with parameters N and CTR.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set Up Credible  AB Test\n",
      "\n",
      "conf = 0.95\n",
      "\n",
      "#in this case we need to estimate the max run time\n",
      "mde = 0.05 \n",
      "iters = 100\n",
      "p_better, p_same, times = expected_results(CredibilityABTest,[SimStream(p_B.lift(mde)), SimStream(p_B), interval, float('inf'), conf], iters)\n",
      "print \"P(Choose A):\",  p_better\n",
      "print \"Mean Run time: \", np.mean(times)\n",
      "print \"STD run Time: \", np.std(times)\n",
      "max_run = np.percentile(times, 95)\n",
      "print \"Max Run Time: \", max_run\n",
      "\n",
      "iters = 200\n",
      "expected_results_by_lift(CredibilityABTest,[SimStream(p_A), SimStream(p_B), interval, max_run, conf], iters, p_hat, lifts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set Up Cost  AB Test\n",
      "\n",
      "cost = 0.001\n",
      "max_run = float('inf')  #this one is so clean it doesnt need a max_run arg\n",
      "\n",
      "iters = 200\n",
      "expected_results_by_lift(CostABTest,[SimStream(p_A), SimStream(p_B), interval, max_run, cost], iters, p_hat, lifts)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set Up NH  AB Test\n",
      "mde = 0.05\n",
      "alpha = 0.05\n",
      "power = 0.95\n",
      "max_run = samples_per_branch_calculator(p_hat, mde, alpha, power)\n",
      "print max_run\n",
      "\n",
      "iters = 200\n",
      "expected_results_by_lift(NHABTest,[SimStream(p_A), SimStream(p_B), interval, max_run, alpha], iters, p_hat, lifts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.random import binomial, multinomial, beta, dirichlet\n",
      "import matplotlib.pyplot as plt\n",
      "a_dist =  beta(7500,  378250, 1000000)\n",
      "b_dist =  beta(7500,  378250, 1000000)\n",
      "\n",
      "#print (a_dist - b_dist)/b_dist\n",
      "#pd.Series((a_dist - b_dist)/b_dist).apply(lambda x: max(x, 0) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lift_d = (a_dist - b_dist)/b_dist\n",
      "plt.hist((a_dist - b_dist)/b_dist, bins = 100)\n",
      "cost_d = pd.Series((a_dist - b_dist)/b_dist).apply(lambda x: max(x, 0) )\n",
      "plt.hist(cost_d, bins = 100)\n",
      "print \"Cost of Choosing B:\", cost_d.mean()\n",
      "print \"A_mean \", a_dist.mean()\n",
      "print \"B_mean:\", b_dist.mean()\n",
      "print \"P(B_Better):\", (b_dist>a_dist).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    }
   ],
   "metadata": {}
  }
 ]
}